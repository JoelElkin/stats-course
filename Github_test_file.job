#!/bin/bash
#! Name of the job:
#SBATCH -J PlotGxP
#! Which project should be charged:
#SBATCH -A ESANTOS-SL3-CPU
#! Which partition/cluster am I using?
#SBATCH -p skylake
#! How many nodes should be allocated? If not specified SLURM
#! assumes 1 node.
#SBATCH --nodes=1
#! How many tasks will there be in total? By default SLURM 
#! will assume 1 task per node and 1 CPU per task.
#SBATCH --ntasks-per-node=1
#! How much memory in MB is required _per node_? Not setting this
#! as here will lead to a default amount per task.
#! Setting a larger amount per task increases the number of CPUs.
#SBATCH --mem=5G  ####--mem-per-cpu
#! How much wallclock time will be required?
#SBATCH --time=00:25:00
#! Work directory (i.e. where the job will run):
workdir="$SLURM_SUBMIT_DIR"

eval "$(conda shell.bash hook)"
conda activate /home/je416/miniconda3/envs/R

ulimit -n 65536
# geno_v_pheno.R <doGeno 8 file> <phenotype file> <SNP name> <phenotype name> <outfile prefix> 

Rscript geno_by_pheno.R ../../output/maylandia_n91_posteriors.$1.$2.geno.gz \
../../data/maylandia_n91_theis_values.txt $1_$2 TheisCount GxP_Theis_count.$1_$2 ../../data/maylandia_PC_covariates_1%.list >> maylandia_n91_theis_values.OutSum 
